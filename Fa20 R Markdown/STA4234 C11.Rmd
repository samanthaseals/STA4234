---
title: 'STA4234: Chapter 10 Examples'
date: "Last updated: `r Sys.Date()`"
output: 
  html_document:
      toc: TRUE
      toc_depth: 5
      toc_float: TRUE
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(pander)


### p.value.string v2
# Update v2: added the formatting that turns of scientific notation
# fixes the case when p = 0.0001 (instead of p=1e-4)
# This function called p.value.string creates a string
# to be used when reporting the p-value. It includes the p on the LHS.
# You will likely place it in $$ to get the LaTeX math formatting.
# This should be placed in an r code chunk in the beginning of your R markdown
# This code chunk by itself shouldn't produce any result.
# by Reid Ginoza

p.value.string = function(p.value){
  p.value <- round(p.value, digits=4)
  if (p.value == 0) {
    return("p < 0.0001")
  } else {
    return(paste0("p = ", format(p.value, scientific = F)))
  }
}
```

## Example 1

The following data are systolic blood pressure (*Y*), sex (*X*<sub>1</sub>), and age (*X*<sub>2</sub>) for a sample of 69 adults. 

### Model 1

```{r, warning}
# read in data
one <- read_csv("/Volumes/GoogleDrive/My Drive/STA4234/R Markdown/SBP.csv")

# we specify an interaction between two variables by joining them with a colon (:)
one_m1 <- lm(sbp ~ age + male + age:male, data=one)
one_c1 <- coefficients(one_m1)
one_s1 <- summary(one_m1)
one_ci1 <- as_tibble(confint(one_m1, level=0.95))
one_t1 <- as_tibble(one_s1[[4]])
```

The resulting regression model is 
\[ \hat{Y} = `r round(one_c1[[1]], digits = 2)` + `r round(one_c1[[2]], digits=2)`X_{\mbox{age}} + `r round(one_c1[[3]], digits=2)`X_{\mbox{male}} `r round(one_c1[[4]], digits=2)`X_{\mbox{age*male}} \]

We can split this into two models -- one for males and one for females. 

For males:
\[ \hat{Y} = 97.08 + 0.95X_{\mbox{age}} \]

For females:
\[ \hat{Y} = 110.04 + 0.96X_{\mbox{age}} \]

Thus, for every year increase in age, we expect SBP to increase by 0.95 mmHg for males and 0.96 mmHg for females.

#### Omnibus Test

**Hypotheses**

&emsp;&emsp; $H_0: \ \beta_1 = \beta_2 = \beta_3 = 0$ <br>
&emsp;&emsp; $H_1:$  at least one $\beta_i \ne 0$

**Test Statistic**

&emsp;&emsp; $F_0 = `r round(one_s1[[10]][1], digits=2)`$.

***p*-value**

&emsp;&emsp; $`r p.value.string(pf(one_s1[[10]][1], one_s1[[10]][2], one_s1[[10]][3], lower.tail = FALSE))`$.

**Rejection Region**

&emsp;&emsp; Reject if $p < \alpha$, where $\alpha=0.05$.

**Conclusion and Interpretation**

&emsp;&emsp; Reject $H_0$. There is sufficient evidence to suggest that the regression line is significant.

#### Individual Tests of Predictors

We noted earlier that the slopes were 0.95 mmHg and 0.96 mmHg for males and females, respectively. Are these slopes really that different? (Do we care about a difference of 0.01 mmHg?) To determine that, we must test $\beta_{\mbox{age*male}}$.

|  Predictor  |           Estimate of $\beta$           |                                    95% CI for $\beta$                                   |                *p*-value                |
|:-----------:|:----------------------------------:|:---------------------------------------------------------------------------------------:|:---------------------------------------:|
| Age         | `r round(one_c1[[2]], digits = 2)` | (`r round(one_ci1$"2.5 %"[2], digits = 2)`, `r round(one_ci1$"97.5 %"[2], digits = 2)`) | `r p.value.string(one_t1$"Pr(>|t|)"[2])` |
| Male sex    | `r round(one_c1[[3]], digits = 2)` | (`r round(one_ci1$"2.5 %"[3], digits = 2)`, `r round(one_ci1$"97.5 %"[3], digits = 2)`) | `r p.value.string(one_t1$"Pr(>|t|)"[3])` |
| Interaction | `r round(one_c1[[4]], digits = 2)` | (`r round(one_ci1$"2.5 %"[4], digits = 2)`, `r round(one_ci1$"97.5 %"[4], digits = 2)`) | `r p.value.string(one_t1$"Pr(>|t|)"[4])` |

We cannot interpret/pay attention to the tests for the slopes for individual predictors (i.e., $\beta_{\mbox{age}}$ and $\beta_{\mbox{male}}$) when an interaction is in the model. We can see that the interaction is not significant (`r p.value.string(one_t1$"Pr(>|t|)"[4])`), thus, we should drop it from our model.

#### Graph

```{r}

# make R see sex as a categorical variable
one$male_fac <- as.factor(one$male) 

# create predicted values for males (male=1) and females (male=0)
one$pred_male <- one_c1[[1]] + one_c1[[2]]*one$age + one_c1[[3]] + one_c1[[4]]*one$age
one$pred_female <- one_c1[[1]] + one_c1[[2]]*one$age

p <- ggplot(one, aes(x=age, y=sbp, color=male_fac)) +
  geom_point() +
  geom_text(aes(x = 74.9, y = 178, label = "Females"), color="black", show.legend = FALSE) + 
  geom_text(aes(x = 73.5, y = 164, label = "Males"), color="black", show.legend = FALSE) +
  geom_line(aes(y = pred_male), color = "black", linetype = "solid") +
  geom_line(aes(y = pred_female), color = "black", linetype = "solid") +
  xlab("Age") + xlim(15, 80) +
  ylab("SBP") +
  scale_color_discrete(name = "Sex", labels = c("Female", "Male")) +
  theme_bw() 

p
```

### Model 2

```{r}
# remove the interaction because it is not significant
one_m2 <- lm(sbp ~ age + male, data=one)
one_c2 <- coefficients(one_m2)
one_s2 <- summary(one_m2)
one_ci2 <- as_tibble(confint(one_m2, level=0.95))
one_t2 <- as_tibble(one_s2[[4]])
```

The resulting regression model is 
\[ \hat{Y} = `r round(one_c2[[1]], digits = 2)` + `r round(one_c2[[2]], digits=2)`X_{\mbox{age}} `r round(one_c2[[3]], digits=2)`X_{\mbox{sex}} \]

We can still split this into two models -- one for males and one for females -- but now we do not have to worry about the interaction term. 

For males:
\[ \hat{Y} = 96.78 + 0.96X_{\mbox{age}} \]

For females:
\[ \hat{Y} = 110.29 + 0.96X_{\mbox{age}} \]

Now that we do not have the interaction term, we see that the slope for age is the same between the two models -- the difference is their intercepts.

#### Omnibus Test

**Hypotheses**

&emsp;&emsp; $H_0: \ \beta_1 = \beta_2 = 0$ <br>
&emsp;&emsp; $H_1:$  at least one $\beta_i \ne 0$

**Test Statistic**

&emsp;&emsp; $F_0 = `r round(one_s2[[10]][1], digits=2)`$.

***p*-value**

&emsp;&emsp; $`r p.value.string(pf(one_s2[[10]][1], one_s2[[10]][2], one_s2[[10]][3], lower.tail = FALSE))`$.

**Rejection Region**

&emsp;&emsp; Reject if $p < \alpha$, where $\alpha=0.05$.

**Conclusion and Interpretation**

&emsp;&emsp; Reject $H_0$. There is sufficient evidence to suggest that the regression line is significant.

#### Individual Tests of Predictors

We now are interested in determining if age and sex are significant predictors of SBP.

|  Predictor  |           Estimate of $\beta$           |                                    95% CI for $\beta$                                   |                *p*-value                |
|:-----------:|:----------------------------------:|:---------------------------------------------------------------------------------------:|:---------------------------------------:|
| Age         | `r round(one_c2[[2]], digits = 2)` | (`r round(one_ci2$"2.5 %"[2], digits = 2)`, `r round(one_ci2$"97.5 %"[2], digits = 2)`) | `r p.value.string(one_t2$"Pr(>|t|)"[2])` |
| Male sex    | `r round(one_c2[[3]], digits = 2)` | (`r round(one_ci2$"2.5 %"[3], digits = 2)`, `r round(one_ci2$"97.5 %"[3], digits = 2)`) | `r p.value.string(one_t2$"Pr(>|t|)"[3])` |

We see that both age and sex significantly predict SBP (`r p.value.string(one_t2$"Pr(>|t|)"[2])` and `r p.value.string(one_t2$"Pr(>|t|)"[3])`, respectively).

#### Graph

```{r}

# create predicted values for males (male=1) and females (male=0)
one$pred_male <- one_c2[[1]] + one_c2[[2]]*one$age + one_c2[[3]]
one$pred_female <- one_c2[[1]] + one_c2[[2]]*one$age

p <- ggplot(one, aes(x=age, y=sbp, color=male_fac)) +
  geom_point() +
  geom_text(aes(x = 74.9, y = 178, label = "Females"), color="black", show.legend = FALSE) + 
  geom_text(aes(x = 73.5, y = 164, label = "Males"), color="black", show.legend = FALSE) +
  geom_line(aes(y = pred_male), color = "black", linetype = "solid") +
  geom_line(aes(y = pred_female), color = "black", linetype = "solid") +
  xlab("Age") + xlim(15, 80) +
  ylab("SBP") +
  scale_color_discrete(name = "Sex", labels = c("Female", "Male")) +
  theme_bw() 

p
```



## Example 2

Commercially produced ice cream is made from a mixture of ingredients: a minimum of 10% milk fat,  9--12% milk solids, 12--16% sweetener, 0.2--0.5% stabilizers and emulsifiers, and 55--64% water. Air is incorporated with the above ingredients during the mixing process. The finest ice creams have between 3% and 15% air.

A food scientist is investigating how varying the amounts of the above ingredients impacts the sensory rating of the final product. The scientist decides to use three levels of milk fat: 10%, 12%, and 15%; three amounts of air: 5%, 10%, 15%; and two levels of sweeteners: 12%, 16%. 

```{r}
# read in data
two <- read_csv("/Volumes/GoogleDrive/My Drive/STA4234/R Markdown/milkfat.csv")

two_m1 <- lm(Ratings ~ Sweetner + MilkFat + Air + Sweetner:MilkFat + Sweetner:Air + MilkFat:Air + Sweetner:MilkFat:Air, data=two)
two_c1 <- coefficients(two_m1)
two_s1 <- summary(two_m1)
two_ci1 <- as_tibble(confint(two_m1, level=0.95))
two_t1 <- as_tibble(two_s1[[4]])
```
